{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00c58666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sbi \n",
    "\n",
    "# import lal as _lal\n",
    "# from pycbc.waveform import get_fd_waveform\n",
    "# from pycbc.psd import aLIGOZeroDetHighPower\n",
    "# from pycbc.filter import highpass\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "from sbi import analysis as analysis\n",
    "from sbi.analysis import pairplot\n",
    "from sbi.inference import NPE, simulate_for_sbi\n",
    "from sbi.utils import BoxUniform\n",
    "from sbi.utils.user_input_checks import (\n",
    "    check_sbi_inputs,\n",
    "    process_prior,\n",
    "    process_simulator,\n",
    ")\n",
    "\n",
    "from sbi.neural_nets import posterior_nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sbi.neural_nets.embedding_nets import (\n",
    "    FCEmbedding,\n",
    "    CNNEmbedding,\n",
    "    PermutationInvariantEmbedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a248e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take real part \n",
    "def simulator_1d(theta):\n",
    "    # theta = [A, t_c]\n",
    "    f_lower = 20\n",
    "    f_final = 80\n",
    "    delta_f = 1/16\n",
    "    n_freq = int(np.round((f_final - f_lower) / delta_f + 1,0))\n",
    "    \n",
    "    num_sim = theta.shape[0]\n",
    "    #print(theta)\n",
    "\n",
    "    f_array = np.linspace(f_lower, f_final, n_freq)\n",
    "    h_freq = np.zeros(shape = (num_sim, len(f_array)))\n",
    "    for i in range(num_sim):\n",
    "        Alog = theta[i,0].numpy()\n",
    "        A = 10 ** Alog\n",
    "        tc = theta[i,1].numpy()\n",
    "        \n",
    "        #print(tc, tc.type())\n",
    "        \n",
    "        h_freq[i, :] = (A * f_array ** (-7/6) * np.e ** (2j * np.pi * f_array * tc)).real\n",
    "    \n",
    "    h_freq = torch.tensor(h_freq, dtype=torch.float32)\n",
    "    return h_freq\n",
    "\n",
    "\n",
    "# normalization for every frequency\n",
    "def normalize_1d(h_freq):\n",
    "    mean = h_freq.mean(dim=0, keepdim=True)  # Mean per frequency bin\n",
    "    std = h_freq.std(dim=0, keepdim=True)   # Std per frequency bin\n",
    "    h_freq_normalized = (h_freq - mean) / (std)\n",
    "    return h_freq_normalized, mean, std \n",
    "\n",
    "def normalize_2d(h):\n",
    "    h_freq_normalized = torch.tensor(np.zeros(shape = h.shape))\n",
    "    h_freq_normalized[:,0,:], mean_real, std_real = normalize_1d(h[:,0,:]) \n",
    "    h_freq_normalized[:,1,:], mean_imag, std_imag = normalize_1d(h[:,1,:]) \n",
    "    return h_freq_normalized, mean_real, mean_imag, std_real, std_imag\n",
    "\n",
    "# global normalization\n",
    "def normalize_1d_(h_freq):\n",
    "    mean = h_freq.mean() \n",
    "    std = h_freq.std()\n",
    "    h_freq_normalized = (h_freq - mean) / (std)\n",
    "    return h_freq_normalized, mean, std \n",
    "\n",
    "def normalize_obs_1d(h_obs, mean, std):\n",
    "    h_obs_normalized = (h_obs - mean) / (std)\n",
    "    return h_obs_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fd34a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training neural network. Epochs trained: 108"
     ]
    }
   ],
   "source": [
    "num_sim = 5000\n",
    "Alog_lowerbound = -21\n",
    "Alog_upperbound = -19\n",
    "tc_lowerbound = 0.01\n",
    "tc_upperbound = 0.5\n",
    "\n",
    "prior = BoxUniform(low=torch.tensor([Alog_lowerbound, tc_lowerbound]), \n",
    "                   high=torch.tensor([Alog_upperbound, tc_upperbound]))\n",
    "\n",
    "\n",
    "# Check prior, simulator, consistency\n",
    "prior, num_parameters, prior_returns_numpy = process_prior(prior)\n",
    "simulator_1d = process_simulator(simulator_1d, prior, prior_returns_numpy)\n",
    "check_sbi_inputs(simulator_1d, prior)\n",
    "\n",
    "# Create inference object. Here, NPE is used.\n",
    "#custom_density_estimator = posterior_nn(model = 'zuko_maf', hidden_features = [64]*3, num_transforms = 5)\n",
    "\n",
    "\n",
    "# #generate simulations and pass to the inference object\n",
    "# theta, h_freq = simulate_for_sbi(simulator, proposal=prior, num_simulations=num_sim)\n",
    "# inference = inference.append_simulations(theta, h_freq)\n",
    "\n",
    "\n",
    "theta_prior = prior.sample((num_sim,))\n",
    "h_sim = simulator_1d(theta_prior)\n",
    "\n",
    "# Normalize the parameters in the prior\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(theta_prior.numpy())\n",
    "theta_prior_normalized = torch.tensor(scaler.transform(theta_prior.numpy()))\n",
    "#theta = scaler.inverse_transform(theta_normalized)\n",
    "\n",
    "# Normalize the data\n",
    "h_sim_normalized, mean, std = normalize_1d_(h_sim)\n",
    "\n",
    "# prior norm that includes +-3sigma\n",
    "prior_norm = BoxUniform(low=torch.tensor([-3, -3]), \n",
    "                        high=torch.tensor([3, 3]))\n",
    "\n",
    "custom_density_estimator = posterior_nn(model = 'zuko_maf', hidden_features = [64]*3, num_transforms = 5)\n",
    "#inference = NPE(prior=prior_norm, density_estimator='zuko_maf')\n",
    "inference = NPE(prior=prior_norm, density_estimator=custom_density_estimator)\n",
    "inference = inference.append_simulations(theta_prior_normalized, h_sim_normalized)\n",
    "\n",
    "\n",
    "# # train the density estimator and build the posterior\n",
    "density_estimator = inference.train()\n",
    "posterior = inference.build_posterior(density_estimator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e2e9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_true = prior.sample((1,))\n",
    "# generate our observation\n",
    "h_obs = simulator_1d(theta_true)\n",
    "h_obs_normalized = normalize_obs_1d(h_obs, mean, std)\n",
    "\n",
    "samples_normalized = posterior.sample((10000,), x=h_obs_normalized)\n",
    "\n",
    "# Inverse the normalization of the parameters\n",
    "samples = torch.tensor(scaler.inverse_transform(samples_normalized.numpy()))\n",
    "\n",
    "_ = analysis.pairplot(\n",
    "    samples, \n",
    "    limits=[[Alog_lowerbound, Alog_upperbound], \n",
    "            [tc_lowerbound, tc_upperbound]],\n",
    "    figsize=(8, 8),\n",
    "    labels=[r\"$A$\", r\"$t_c$\"],\n",
    "    points=theta_true # add ground truth thetas,\n",
    ")\n",
    "\n",
    "# plt.savefig(\"Gravitational Wave Posterior\")\n",
    "# plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84903335",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
